<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>Vishal Rajpal</title>
  <generator uri="https://github.com/jekyll/jekyll">Jekyll v3.1.1</generator>
    <icon>http://vishalrajpal25.github.io/apple-touch-icon-precomposed.png</icon>
  <subtitle>Big data, Hadoop, Spark, moving to Chicago, fitness</subtitle>
  <link href="http://vishalrajpal25.github.io/atom.xml" rel="self"/>
  <link href="http://vishalrajpal25.github.io/" rel="alternate" type="text/html"/>
  <updated>2016-04-17T22:59:49-05:00</updated>
  <id>http://vishalrajpal25.github.io/</id>
  <author>
    <name>Vishal Rajpal</name>
    <uri>http://vishalrajpal25.github.io/</uri>
    <email>vishalrajpal25@gmail.com</email>
  </author>

  
  <entry>
    <title>Data, Model, Score, Decision - Time-to-market for Big Data Analytics</title>
    <link href="http://vishalrajpal25.github.io/bigdata/data-model-score-decision-time-to-market-for-big-data-analytics/"/>
    <updated>2016-04-17T22:32:35-05:00</updated>
    <id>http://vishalrajpal25.github.io/bigdata/data-model-score-decision-time-to-market-for-big-data-analytics</id>
    <author>
      <name>Vishal Rajpal</name>
      <uri>http://vishalrajpal25.github.io/</uri>
      <email>vishalrajpal25@gmail.com</email>
    </author>
    <content type="html">
      
      &lt;p&gt;One of the known challenges for Big data analytics has been the time-to-market OR production deployment of analytical models and solutions.&lt;/p&gt;

&lt;h3&gt;What does that mean:&lt;/h3&gt;

&lt;p&gt;Organizations have multiple problems to solve which impact their bottom and top-line. Various analytical solutions (descriptive and predictive) are developed to optimize their operations. Once these solutions are ready to predict the best possible business outcomes, the next step is to operationalize them in a production environment. These solutions can then learn as well as predict on an ongoing basis at a predetermined frequency. This frequency could be yearly, daily, monthly or even near real-time (as the data comes in) basis.
The challenge:&lt;/p&gt;

&lt;p&gt;Different departments and teams (internal and external) within an organization develop these solutions using their platform of choice. They have their own standards to write the solution code.
To summarize, the differences lie in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Platform / technology for the model development&lt;/li&gt;
&lt;li&gt;Data pull standard&lt;/li&gt;
&lt;li&gt;Data pre-processing mechanism&lt;/li&gt;
&lt;li&gt;Output score generation&lt;/li&gt;
&lt;li&gt;Output publishing for decision making and consumption&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These differences give rise to the challenge of deploying unrelated and complex models in production. In certain cases, it has been observed that the time to deploy and enhance a model has been equal to the time it took to build the model. A large investment of time and money is needed before the solutions can really start producing the value they were built for.&lt;/p&gt;

&lt;h3&gt;So what can be done:&lt;/h3&gt;
&lt;p&gt;Analytics operationalization can be made faster and efficient through a combination of automation, processes and practices.&lt;/p&gt;

&lt;ol&gt;
&lt;b&gt;&lt;li&gt;Input data pipeline&lt;/li&gt;&lt;/b&gt;

The data pulled to build a model in Hadoop, SAS, python or R is generally a subset of the overall data. Thus, IT or data science team pull the data in an ad-hoc manner without following any standards. There should be a pipeline mechanism developed at the start of the first pulls so that subsequent pulls - whether meant for development or ongoing prediction can be configured. This saves a lot of time since a new data source pull can easily be configured rather than developed from scratch. Investing in a long-term data pipeline pays off as the number of data sources and analytic solutions increase.

&lt;br /&gt;&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Code practices&lt;/li&gt;&lt;/b&gt;
It is very important that data scientists, engineers and IT team members develop code which can easily be deployed. Leaders should motivate team members to follow standard practices like using parameters wherever possible from the early stages of code development. Some standard parameters could be date, month, year, environment, output, etc.
This practice can save a lot of time and effort once the models are ready to be productized. 

&lt;br /&gt;&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Framework for operationalization&lt;/li&gt;&lt;/b&gt;

An operationalization framework helps drive faster deployments of models and analytic solutions.
This framework should provide the flexibility to configure :
&lt;ul&gt;
&lt;li&gt;input data pipelines&lt;/li&gt;
&lt;li&gt;save and re-load models&lt;/li&gt;
&lt;li&gt;save and load meta-data&lt;/li&gt;
&lt;li&gt;configure and run rules (like a rules engine)&lt;/li&gt;
&lt;li&gt;publish the output to multiple enterprise systems like databases, ERP systems or even enterprise message bus&lt;/li&gt;
&lt;/ul&gt;

&lt;/ol&gt;

&lt;h3&gt;Existing tools&lt;/h3&gt;

&lt;p&gt;Apache Oozie is a great job and workflow scheduling tool. It provides the flexibility to configure varied types of jobs like Hive, python, spark, hive, shell to name a few. But with greater flexibility comes larger responsibility for the analysts. Each analyst needs to develop their own data sourcing, model re-load and output push mechanism. Building a framework as proposed above, all the repetitive and important steps can be standardized, thus helping save valuable resources.&lt;/p&gt;

&lt;p&gt;Eventually, the “V”ALUE generation is efficient and achieved faster.&lt;/p&gt;

&lt;p&gt;In the next series of posts, we will take a detailed look at a proposed design of the operationalization framework. We will also try socialize the design with the Apache Oozie community for their consideration and feedback.&lt;/p&gt;

    </content>
  </entry>
  
  <entry>
    <title>Machine Learning - Spark Over Mahout</title>
    <link href="http://vishalrajpal25.github.io/bigdata/machine-learning-spark-over-mahout/"/>
    <updated>2016-03-30T10:34:56-05:00</updated>
    <id>http://vishalrajpal25.github.io/bigdata/machine-learning-spark-over-mahout</id>
    <author>
      <name>Vishal Rajpal</name>
      <uri>http://vishalrajpal25.github.io/</uri>
      <email>vishalrajpal25@gmail.com</email>
    </author>
    <content type="html">
      
      &lt;p&gt;I started my career writing code in Java. I definitely love the object oriented language and the GC support. I experienced the native side of Java while implementing few low latency high throughput applications at a financial institution. The power is immense.&lt;/p&gt;
&lt;p&gt;
As I started developing machine learning models in Hadoop, Apache Mahout was my straight forward choice since it was natively implemented in Java. The problem I tried to solve was to build a recommendation system for users. Like any Java developer, I looked at the available API implementations and tried to map it to the algorithm I needed to use OR was advised to use. 
&lt;p&gt;
Today, Mahout has implementations availble across Spark, H2O and Flink. At the time while I was using Mahout, it had single machine and map reduce implementations.
Mahout does have a very rich API for algorithm implementations across Collaborative Filtering, Classification, Clustering and even Dimensionality reduction, found &lt;a href=&quot;https://mahout.apache.org/users/basics/algorithms.html&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.
I used the user based collaborative filtering with the cosine heuristic. The feedback was not explicit (ratings) but an implicit one (preference). Thus, I had to extend some of the implementation classes to customize for cosine similarity evaluation while generating the recommendation.
&lt;p&gt;
Once that completed, the recommendations were cooked in Mahout. The next time I used Spark to develop a analytical solutions. As I look back at the experiences while using Mahout and Spark, the reasons the makes Spark better at a tactical user level IMHO are:

&lt;ol&gt;
&lt;li&gt;Native language support&lt;/li&gt;
Mahout is entirely implemented in Java, hence any enhancements or extensions to the existing API needs to be done in Java. Spark on the other hand is implemented in Scala, but has an exposed API in Python, Java and R. This is a very important feature - I will give you two scenarios as example - 
&lt;ul&gt;
&lt;li&gt;Imagine while data pre-processing you need a function or feature which is implemented in R or python through their open source APIs. This feature may not be implemented in Java and thus cannot be leveraged in Mahout.&lt;/li&gt;
&lt;li&gt;During model operationalization, it is faster to develop, score and update using the same set of API through python or R.&lt;/li&gt;
&lt;/ul&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;li&gt;Development time&lt;/li&gt;
Through Python and Scala, Spark provides a command shell which is very handy during model development phase. You may need to pre-process data through multiple steps, try and re-iterate over steps and have the flexibility to test alternative options without re-compiling the code. Mahout does not have that flexibility since any code that you enhance needs to be re-compiled to be executed again. This takes a lot of time and I think Spark kills it there.
&lt;br /&gt;&lt;br /&gt;
&lt;li&gt;Debugging&lt;/li&gt;
While development, it is important to have the flexibility to debug, particularly when you are dealing with large scale data. Mahout code written in Java and may be running on Spark or Map reduce paradigm is difficult to debug. You may end up writing logs after each step in your java code and trust me, it is very difficult to debug distributed code. Spark provides compilation of every step and interpreter shell for each step, hence it becomes easy to find root cause of bugs faster.
&lt;br /&gt;&lt;br /&gt;
&lt;li&gt;Advancements&lt;/li&gt;
Spark has more contributors and the MLLib is maturing very fast. Other open source contributors are also building secondary libraries outside Spark for easier use, for example sparkit-learn which is a Spark based implementation of scikit-learn.
&lt;/ol&gt;
These are all practical development advantages of Spark and have nothing to do with technical or architectural comparisons of map reduce or spark framework.
&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;

    </content>
  </entry>
  
  <entry>
    <title>Hadoop Data Loading - The Beauty in Sqoop&#39;ing&#39;</title>
    <link href="http://vishalrajpal25.github.io/bigdata/hadoop-data-loading-the-beauty-in-sqooping/"/>
    <updated>2016-03-30T10:32:59-05:00</updated>
    <id>http://vishalrajpal25.github.io/bigdata/hadoop-data-loading-the-beauty-in-sqooping</id>
    <author>
      <name>Vishal Rajpal</name>
      <uri>http://vishalrajpal25.github.io/</uri>
      <email>vishalrajpal25@gmail.com</email>
    </author>
    <content type="html">
      
      &lt;p&gt;Almost all analytic use cases utilize internal enterprise data, which is stored in traditional relational database. The enterprise data from the relational store needs to be pulled to Hadoop for model development as well as during operationalization for ongoing scoring.&lt;/p&gt;
&lt;p&gt;
Apache Sqoop(TM) is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases. Sqoop is fast since it pulls and loads data in parallel using the map reduce framework, i.e. mapper and reducer jobs are launched to pull data in splits from the relational store and load it in parallel in HDFS. It can load data either in HDFS or directly create table in Hive based on the schema in the source database. It can connect to any database platform having a jdbc driver available. I feel the open source community have done great work in creating Sqoop and the features that it has through the properties. It is one of the most important tools in a enterprise Big data technology stack from an open source perspective (unless a proprietary ELT tool has not been acquired / invested)
&lt;p&gt;
For the remaining part of the post, I will share an example of loading data from Microsoft SQL server to Hadoop HDFS in avro format. The data can be directly loaded in Hive warehouse as well, but Sqoop does not support Hive table creation with underlying avro format data. Either the data is loaded in HDFS in avro format OR one can create a hive table with underlying data in Text format.

Sqoop command used for loading data from MS-SQL to HDFS in avro format:
&lt;code&gt;
sqoop import --connect &quot;jdbc:jtds:sqlserver://&amp;lt;HOSTNAME&amp;gt;:&amp;lt;PORT&amp;gt;;userNTLMv2=true;domain=&amp;lt;DOMAIN&amp;gt;;database=&amp;lt;DBNAME&amp;gt;&quot;&lt;br /&gt; --username &amp;lt;user&amp;gt; --driver &#39;net.sourceforge.jtds.jdbc.Driver&#39; -e &amp;lt;QUERY&amp;gt; --as-avrodatafile --target-dir &amp;lt;HDFS_TARGET_DIR&amp;gt; &lt;br /&gt;--split-by &amp;lt;column_name&amp;gt;--class-name &amp;lt;NAME_OF_FILE&amp;gt; --map-column-java &amp;lt;column=type&amp;gt;
&lt;/code&gt;
&lt;br /&gt;&lt;br /&gt;
Let&#39;s look at the important properties and tweaks:
&lt;ol&gt;
&lt;li&gt;Connect String for MS-SQL (--connect)&lt;/li&gt;
Hostname and port are pretty standard information for connecting to the database. Now if you are connecting to a MSSQL server configured withh Windows authentication, you need to configure the property userNTLMv2=true and provide the domain name through domain=DOMAIN. Also, ensure to use the jtds jdbc driver and not the SQL JDBC driver for MS-SQL. Download the jar and ensure that it is available in the classpath for the Sqoop bin command.
&lt;br /&gt;&lt;br /&gt;
&lt;li&gt;Driver&lt;/li&gt;
As mentioned above, in case you have windows authentication for the database connection, use the jtds driver through --driver &#39;net.sourceforge.jtds.jdbc.Driver&#39;.
&lt;br /&gt;&lt;br /&gt;
&lt;li&gt;Format&lt;/li&gt;
Import the data as avro format using --as-avrodatafile.
&lt;br /&gt;&lt;br /&gt;
&lt;li&gt;Split by&lt;/li&gt;
Sqoop splits the data import based on a particular column. Identify the column which will produce equal number of splits, in other words is well distributed to enable the input data be broken into chunks that can be handled by each mapper. For example, I used the date field as the split by column. Sqoop evaluate the total number of splits required based on the min and max values and launches the number of mappers to import the data in parallel. This is a very important property. Alternatively, you can provide Sqoop a query that evaluates min and max type int for determining the splits through property --boundary-query. 
&lt;br /&gt;&lt;br /&gt;
&lt;li&gt;map-column-java&lt;/li&gt;
Key value combination of column name and java type to be cast as while importing.
&lt;/ol&gt;
There are other properties as well based on the source database and destination in Hadoop. 

&lt;p&gt;
Once the data is imported in HDFS as avro format, a schema file is also generated along with it. The name of the file is the class-name property value provided to the Sqoop import command. The extension is avsc.
&lt;p&gt;
Push the avro schema file to a HDFS file directory using command:
&lt;code&gt;hdfs -put &amp;lt;avro_file.avsc&amp;gt; &amp;lt;HDFS_DIR&amp;gt;&lt;/code&gt;
&lt;p&gt;
OR
&lt;p&gt;
&lt;code&gt;hdfs -copyFromLocal &amp;lt;avro_file.avsc&amp;gt; &amp;lt;HDFS_DIR&amp;gt;&lt;/code&gt;
&lt;p&gt;
Create Hive table using the avro data file and avro schema file:
&lt;code&gt;CREATE EXTERNAL TABLE TABLENAME
ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.avro.AvroSerDe&#39; &lt;br /&gt;
STORED AS AVRO
LOCATION &#39;hdfs://&amp;lt;AVRO_DATA_FILE&amp;gt;&#39;
TBLPROPERTIES (&#39;avro.schema.url&#39;=&#39;&amp;lt;AVRO_SCHEMA_FILE.avsc&amp;gt;&#39;);
&lt;/code&gt;
&lt;p&gt;
Happy Sqoop&#39;ing&#39;!
&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;

    </content>
  </entry>
  
  <entry>
    <title>Beat Your Sedentary Lifestyle</title>
    <link href="http://vishalrajpal25.github.io/wellbeing/beat-your-sedentary-lifestyle/"/>
    <updated>2016-03-22T13:32:56-05:00</updated>
    <id>http://vishalrajpal25.github.io/wellbeing/beat-your-sedentary-lifestyle</id>
    <author>
      <name>Vishal Rajpal</name>
      <uri>http://vishalrajpal25.github.io/</uri>
      <email>vishalrajpal25@gmail.com</email>
    </author>
    <content type="html">
      
      &lt;p&gt;Exploding belly and abdomen side fat is a common problem today due to the sedentary lifestyle most of us have. We always keep making promises, procastinating or paying for gym memberships to burn the fat, but in vain.
I think there are two biggest hurdles to a health lifestyle - the beginning and then, maintaining the consistent efforts. It is not a one hour or a day activity, it requires consistent perseverance and efforts. 
The good part is once you have crossed a few days following a disciplined lifestyle, there are very high odds that you would continue leading the healthy lifestyle. No one is perfect, I myself struggle everyday fighting the mental excuses to avoid exercising. In the remaining part of the post, I will try to outline small steps to start with and then scaling them as one progresses in the journey of being healthy and fit.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Eating habits&lt;/li&gt;
One of the biggest reasons of being unhealthy and gaining weight is what we eat. Altering a few eating habits can make huge changes to the way you feel. It is not always about quitting to eat sweetened food, colas, beer, cheeze burgers, etc. It is about taking small steps. One can give up at least one of those hazardous foods a week at a time. The first day might hurt, but the following days will make you feel much better. One may realize that the pleasure of not giving up to the urge is much more as compared to the that on giving in. I gave up my post dinner desert one night - an urge converted to an addiction over the last few years. I do give in a number of times, but still try to consciously avoid whenever I can. Pick one of fatty food your choice and avoid it as far as you can. You may enjoy the journey and start having healthy foods at the right time during the day. Who knows, you may succeed and it takes you a long way.
&lt;li&gt;Cardio exercise&lt;/li&gt;
Cardio a.k.a. aerobic exercise is any form of low to high intensity exercise that pumps up your heart beat and makes you require more oxygen. Long distance running, jogging, cycling and even brisk walking qualify as cardio exercise. Post warm up, a decent cycling or running distance helps loosen up those muscles before the strength training. Running and cycling help tone lower body and burn some calories. Personally, I feel swimming not only tones your body but also burns those extra calories faster. Pick your choice.
&lt;li&gt;Strength training&lt;/li&gt;
Strength and endurance training are equally important. Physical exercise is not limited to walking or jogging. An equal amount of attention needs to be given to strength and endurance exercises. These includes working out the muscles of upper, lower and core part of the body like biceps, triceps, abs, calves, chest, back to name a few. If you hit the gym, ensure to spend proportionate (not equivalent) time working out the muscles of your body through strength training as compared to cardio exercise. The proportion of time you spend depends on your age, body needs, etc. Talk to a certified fitness instructor.
&lt;/ol&gt;

&lt;p&gt;Burn those calories and enjoy the cheat diet sometimes!&lt;/p&gt;

    </content>
  </entry>
  
  <entry>
    <title>Applying for a Driver&#39;s License in Chicago</title>
    <link href="http://vishalrajpal25.github.io/diy/applying-for-a-drivers-license-in-chicago/"/>
    <updated>2016-03-21T10:11:10-05:00</updated>
    <id>http://vishalrajpal25.github.io/diy/applying-for-a-drivers-license-in-chicago</id>
    <author>
      <name>Vishal Rajpal</name>
      <uri>http://vishalrajpal25.github.io/</uri>
      <email>vishalrajpal25@gmail.com</email>
    </author>
    <content type="html">
      
      &lt;p&gt;Driver’s license is one of the most important documents that one applies for in the US. Driver’s license not only is your permit to drive motor vehicles, but is a general identity document issued by your state. It contains key information like one’s full name, date of birth, social security # and address, thus can be used as a state ID as well.&lt;/p&gt;

&lt;p&gt;I recently applied for an IL state driver’s license and got it approved. I did not use any help from a learning school OR a friend (for a car), hence thought to share my “DIY” experience.&lt;/p&gt;

&lt;p&gt;Below are the steps for applying a driver’s license in DIY mode:&lt;/p&gt;
&lt;ol&gt;
&lt;b&gt;&lt;li&gt;Documents&lt;/li&gt;&lt;/b&gt;
Go to the IL DMV website and download the PDF which contains the list of documents that one needs to have for applying a permit. There are four categories and one should have 1 document each for 3 categories and 2 documents for the residential address proof. Please ensure that you have the necessary documents available, specifically I-94 in case you are an alien.

&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Written Test&lt;/li&gt;&lt;/b&gt;
Please do not take the written test lightly. Ensure to spend at least a couple of hours going through practice tests, signs and general documentation provided by the DMV. I took 4 practice tests and went through the signs in the PDF provided on the DMV website. Once you are ready, take a sample test from DMV available free with a login. On the test day, you will realize the importance of going through the material and signs. It is not that hard, but is definitely not easy as well. One needs to have some background to answer the questions correctly.

&lt;br /&gt;
&lt;br /&gt;

&lt;b&gt;&lt;li&gt;Driving Test&lt;/li&gt;&lt;/b&gt;
For the driving test, you need a vehicle of your own. The DMV does not provide a vehicle for the driving test. You may either request a friend and borrow his/her vehicle. In that case, the friend needs to accompany you to the DMV and provide his written consent to use his car for the driving test. 
For others, you may need to rent a car with the proper insurance coverage. I rented a vehicle from the ORD airport at 12 USD / day and bought a 13 USD insurance coverage on the vehicle. 
&lt;p&gt;
The driving test is again not that difficult, but do not undermine it as well. Ensure to follow all rules - wear seat belts and adjust mirrors as you sit in the car. Follow all STOP signs which means bring the car to a complete halt at the intersections having STOP signs. 
The evalutor might talk to you during the time of the drive, hence you need to be very alert while driving and communicating with them at the same time. 

&lt;br /&gt;&lt;br /&gt;
Once you successfully pass the tests, a photo is clicked and your signature is requested. That&#39;s it, you have your driver&#39;s license in your hand the same day.

&lt;/p&gt;&lt;/ol&gt;

    </content>
  </entry>
  
  <entry>
    <title>Settling in Chicago</title>
    <link href="http://vishalrajpal25.github.io/diy/settling-in-chicago/"/>
    <updated>2016-03-02T18:37:07-06:00</updated>
    <id>http://vishalrajpal25.github.io/diy/settling-in-chicago</id>
    <author>
      <name>Vishal Rajpal</name>
      <uri>http://vishalrajpal25.github.io/</uri>
      <email>vishalrajpal25@gmail.com</email>
    </author>
    <content type="html">
      
      &lt;p&gt;We (my wife and I) had a unique experience of finding an apartment, buying necessary amenities / furniture at the time we reached Chicago. I thought to document our experiences and share for people who just moved and need a guide.&lt;/p&gt;

&lt;h5&gt;Our Experiences&lt;/h5&gt;
&lt;p&gt;The first challenge began with the weather. Chicago is very cold during the winters - specifically months of January and February. I would strongly recommend to take the cold seriously and wear 4-5 layers while stepping out during cold weather.&lt;/p&gt;

&lt;p&gt;I reached Chicago in the 2nd week of January with high expectations to settle as quickly as possible. The day we reached, the weather department issued a wind chill advisory, which means temperature dropping to single digits in Fahrenheit (negative in celsius). I clearly remember temperature of -17 degrees C on the days we were visiting various apartments in Downtown Chicago.&lt;/p&gt;

&lt;h5&gt;Tips&lt;/h5&gt;
&lt;p&gt;Here are a few tips I thought to share:&lt;/p&gt;

&lt;ol&gt;
&lt;b&gt;&lt;li&gt;Pre-liminary research&lt;/li&gt;&lt;/b&gt;
Although the first step should be to identify one&#39;s budget, it does not harm in browsing around doing some research on the available apartments and their rents. One could use a number of national (apartment, rent, etc) and local (domu, craigslist) websites to check various locations and the rents in those areas,
&lt;br /&gt;&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Budget&lt;/li&gt;&lt;/b&gt;
This is very important. Vanilla case would be 35% of take home salary is the max one can afford to pay as monthly rent. I have a slightly different opinion, since you may have an option to save more and pay only 10-20% of your take home as rent. Also, none of the websites I browsed mentioned about the added costs related to renting an apartment. These are utility bills, renters insurance, one time internet cable connection charges to name a few. Ensure that your budget is inclusive of all the monthly pay-outs apart from the rent to be able to completely visualize your cost of renting the apartment
&lt;br /&gt;&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Location&lt;/li&gt;&lt;/b&gt;
One of the most important criteria is whether to rent in Chicago downtown or suburbs. There are limitless options for one to rent an apartment. Some folks whom I spoke to try to find locations closer to their friends, relatives, god-fathers or god-brothers. Post that, they looked at their location of work. I would recommend both - try to find an apartment closest to work and also reachable to acquaintances. Both are equally important. You may find spending 200 USD more on an apartment closer to work and getting more time with family worthwhile as opposed to saving the 200 USD (which eventually you may spend in buying Metra tickets). Nevertheless, the right location is crucial, save money as well time.
&lt;br /&gt;
Downtown - high rents (-), no need to maintain a car (-), very costly parking (-), easily approachable (+), events and city life (+)
&lt;br /&gt;
Suburbs - low cost of living including rents and grocery (+), free / cheap parking (+), travel time to Chicago city (-)
&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Agents&lt;/li&gt;&lt;/b&gt;
There are some very knowledgable agents who can help you find you the right apartment. What you need to do is call them, provide your budget, preferred location, office location, parking needs, schooling needs and the like. Just share whatever is your criteria and they will take care of the rest. They will necessarily short list 3-4 options, take appointments, drive you around the 4 places and help you select the one.
P.S. Their service is free of cost, they charge the building management for their services on every apartment they help lease out. 
I find it very cool!

&lt;/ol&gt;

&lt;p&gt;Once you finalize the apartment, below are some tips related to setting up your place:&lt;/p&gt;
&lt;ol&gt;
&lt;b&gt;&lt;li&gt;Renter&#39;s agreement&lt;/li&gt;&lt;/b&gt;
Provide necessary documents - passport, visa and family details and the building authorities take care of creating the agreement. It is an electronic agreement, so you need to just go through it and sign electronically.
&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Renter&#39;s insurance&lt;/li&gt;&lt;/b&gt;
It is a pre-requisite to take renters insurance before the possession is handed over to you. It costs around 25-30 USD and is straightforward. You can pick one of the insurance amount specified in your agreement.
&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Advance rent&lt;/li&gt;&lt;/b&gt;
Prepare money order for the advance rent for the first month. I used US Postal service given that it was cheaper as compared to banks.
&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Necessities&lt;/li&gt;&lt;/b&gt;
Buy a NEW Mattress and bed, refrain from buying a used one. Other furniture can be bought used through craigslist OR any local furniture store or website. Electronics like Smart TV as well can be bought used but I bought a new one (that&#39;s my preference).
&lt;/ol&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
Hope the above is helpful. If you have any other specific questions around settling in Chicago, drop me a note and I would be glad to help.&lt;/p&gt;


    </content>
  </entry>
  
  <entry>
    <title>Spark on YARN - Considerations and Configurations</title>
    <link href="http://vishalrajpal25.github.io/bigdata/spark-on-yarn-considerations-and-configurations/"/>
    <updated>2016-02-25T15:29:05-06:00</updated>
    <id>http://vishalrajpal25.github.io/bigdata/spark-on-yarn-considerations-and-configurations</id>
    <author>
      <name>Vishal Rajpal</name>
      <uri>http://vishalrajpal25.github.io/</uri>
      <email>vishalrajpal25@gmail.com</email>
    </author>
    <content type="html">
      
      &lt;h5&gt;About Spark&lt;/h5&gt;
&lt;p&gt;Apache Spark™ is a DAG based execution engine for large-scale data processing. It has in-memory processing capabilities making it faster as compared to other execution engines. Spark boasts ease of use and generality through native language support (R, Scala, Python, Java ) and SQL &amp;amp; streaming analytics respectively.&lt;/p&gt;

&lt;h5&gt;Spark on Yarn&lt;/h5&gt;
&lt;p&gt;Spark runs on Hadoop (YARN), Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.
Running Spark on YARN cluster processng of data available in HDFS as well as leveraging the hardware resources available in the already commissioned Hadoop cluster.&lt;/p&gt;

&lt;p&gt;Spark documentation has detailed information about running Spark engine on YARN &lt;a href=&quot;http://spark.apache.org/docs/latest/running-on-yarn.html&quot; about=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As part of the article, I intend to expand on few of the important considerations and configurations to be aware about during practical usage of Spark for Machine learning or general purpose processing. 
I had the opportunity to play around these configurations and understand the considerations while I was involved in developing a machine learning model on text data. The versions used were: Spark 1.2, Python 2.6 and YARN.&lt;/p&gt;

&lt;ul&gt;
&lt;b&gt;&lt;li&gt;Hadoop environment variables&lt;/li&gt;&lt;/b&gt;
For Spark to run on YARN and also access the HDFS directories, it needs to know the Hadoop and Yarn client properties - basically, the resource manager (RM) address and also the HDfS port.
Below are the environment variables that need to be set to the directory containing the necessary configurations:
&lt;br /&gt;
export HADOOP_CONF_DIR=HADOOP_CONF_DIR
&lt;br /&gt;
export YARN_CONF_DIR=YARN_CONF_DIR
&lt;br /&gt;
&lt;br /&gt;

&lt;b&gt;&lt;li&gt;Initializing Spark on YARN&lt;/li&gt;&lt;/b&gt;
For initializing Spark on YARN, one needs to set the parameter --master to value as yarn. Within YARN, there are two modes - client and cluster. The modes have one significant difference pertaining to the control and execution of the driver program for the Spark application - whether within the client program OR AM (application master) in YARN.
&lt;code&gt;$ $SPARK_BIN/spark-submit --master yarn .... &lt;/code&gt;

&lt;br /&gt;
&lt;br /&gt;

&lt;b&gt;&lt;li&gt;Imports and parameters&lt;/li&gt;&lt;/b&gt;
Spark is a distributed computing engine, where multiple machines are used to run executors working on different tasks. These tasks are part of a single application controlled by the driver. As the executors run tasks in parallel, it is imperative that all executors running on machines across the cluster have access to the configurations, base code, libraries and user defined modules used in the application code. Spark provides multiple parameters to share Java maven, python modules and libraries to all executors part of the launching application. These files can be passed from the client script launching the spark application. The parameters are listed below: &lt;br /&gt;
  &lt;ol&gt;
  &lt;li&gt;--packages              Comma-separated list of maven coordinates of jars &lt;/li&gt;
  &lt;li&gt;--py-files PY_FILES     Comma-separated list of .zip, .egg, or .py files, one might always have some proprietary functions defined and used in the application&lt;/li&gt;
  &lt;li&gt;--files FILES           Comma-separated list of files, general purpose files, dictionaries, etc.&lt;/li&gt;
  &lt;li&gt;--conf PROP=VALUE       Arbitrary Spark configuration property, VM settings, etc.&lt;/li&gt;
  &lt;li&gt;&lt;i&gt;--properties-file**&lt;/i&gt;  Override spark specific properties, like memory limits&lt;/li&gt;
  &lt;/ol&gt;                         
&lt;br /&gt;

&lt;b&gt;&lt;li&gt;Memory related properties&lt;/li&gt;&lt;/b&gt;
Spark provides a number of properties for fine-tuning or changing default configurations. The primary ones that we need for large scale data processing are related to the memory settings of executor and drivers (listed below).

&lt;ol&gt;

&lt;li&gt;driver-memory ::- Total memory for driver (e.g. 1000M, 2G), container level for YARN&lt;/li&gt;
&lt;li&gt;executor-memory  ::- Total nemory per executor (e.g. 1000M, 2G), container level for YARN&lt;/li&gt;
&lt;li&gt;spark.yarn.executor.memoryOverhead ::- Typically 6-10% of executorMemory, with minimum of 384 MB. This is the amount of off-heap memory (in megabytes) to be allocated per executor. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the executor size (typically 6-10%). &lt;/li&gt;
&lt;li&gt;spark.yarn.driver.memoryOverhead ::- Typically 6-10% of driverMemory, with minimum of 384 MB. This is the amount of off-heap memory (in megabytes) to be allocated per driver in cluster mode. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the container size. &lt;/li&gt;
&lt;li&gt; spark.yarn.am.extraJavaOptions ::- A string of extra JVM options to pass to the YARN Application Master in client mode. One can use it to pass gC level setting or debugging parameters.&lt;/li&gt;


&lt;h5&gt;Closing&lt;/h5&gt;
Spark is a powerful in-memory data processing engine with a rich API and advanced data processing capabilities like graph and machine learning. One does need to understand the right combination of Spark configurations and their tuning criteria to be able to pre-process, process and train a model on large data sets.
A typical pyspark of scala spark launcher command might look like (based on the content of the article)::
&lt;br /&gt;
&lt;code&gt;pyspark --master yarn --py-files pre_processing.py --conf spark.driver.maxResultSize=1g --num-executors 10 --driver-memory 1g --executor-memory 768m &lt;/code&gt;


&lt;/ol&gt;&lt;/ul&gt;

    </content>
  </entry>
  
  <entry>
    <title>Big Data 2.016 - Practical implementation challenges</title>
    <link href="http://vishalrajpal25.github.io/bigdata/2016-real-challenges-in-big-data-implementations/"/>
    <updated>2016-02-23T11:10:06-06:00</updated>
    <id>http://vishalrajpal25.github.io/bigdata/2016-real-challenges-in-big-data-implementations</id>
    <author>
      <name>Vishal Rajpal</name>
      <uri>http://vishalrajpal25.github.io/</uri>
      <email>vishalrajpal25@gmail.com</email>
    </author>
    <content type="html">
      
      &lt;p&gt;We have all witnessed the journey of “Big data” from inception until 2015. Big data got a fair share of attention and has passed through the hype cycle. In 2016, there is a different set of challenges that organizations need to overcome for realizing the true potential of Big data through analytics. These challenges are primarily related to implementation of Big Data projects.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Until 2015&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;By the end of 2015, most organizations would have already invested in some pilot Big data infrastructure (appliance, datacenter, hadoop, NoSQL store)  and might have executed a few proof-of-concept projects. These projects would have helped provide an idea about the complexity and investment required for implementing Big data projects.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;2016 &amp;amp; beyond&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;2016 marks the phase of implementing advanced analytical solutions in the Big data infrastructure. This is the time when the business value from Big data is realized. Below are few of the challenges that would be part of (at least) the 2016 journey as the implementations become real.&lt;/p&gt;
&lt;ol&gt;
&lt;b&gt;&lt;li&gt;Infrastructure&lt;/li&gt;&lt;/b&gt;
As implementations are productionized, there arises a need for separate environments for development, staging / testing and production within enterprise systems. The challenge would be to acquire, plug and commission the right infrastructure to roll out within the organization. There is a fair amount of estimation required to determine the hardware (nodes / racks) based on the workload to be supported in the longer term. Along with the hardware, determining the appropriate versions of operating systems, VMs&#39; and tools might take time and investment. Once the infrastructure and tools are determined, it needs to be integrated within the data centers and other existing enterprise systems.
&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Security &amp;amp; Governance&lt;/li&gt;&lt;/b&gt;
There are mechanisms available for securing the Big data environment at various levels -  network, storage encryption and access. Integrating the security tools with the enterprise user directory and authentication protocols is challenging due to the maturity of the said security tools. Also, before we begin the ingestion of data in the environment, it is important to define the governed areas for hosting data per the type - raw, mapped, processed, production runs and so on. The defined zones need to be created with appropriate policies based on user / team and application access.
&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Data sourcing&lt;/li&gt;&lt;/b&gt;
Pulling data in the data lake for analysis has been one of the biggest challenges. Almost all analytic use cases use enterprise data. With the polygot persistence pattern, pulling data from system of record, data warehouse, raw files and MPP systems is a daunting task. Ingesting data from multiple sources / teams requires a lot of coordination and effort for project management to ensure smooth execution. The effort might require creation of external views in existing systems to enable data pull in the cluster using ingestion tools. Also, if the analysis require maintaining a bi-temporal view of the data (snapshots), it becomes much more difficult if the source systems do not have history of change or audits to maintain changes. 

&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Processes&lt;/li&gt;&lt;/b&gt;
Execution of Big data projects involves multiple teams (IT, data engineers, data analysts, data scientists, business owners) . Processes and handshakes are required to streamline project execution and avoid project delays.  Execution workflows need to be defined for project initiation, data source pull, model development standards, data sharing and operationalization. The organization structure within an enterprise drives the processes being created and owners of those processes.

&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Standardization&lt;/li&gt;&lt;/b&gt;
Data being available from disparate sources calls for standardization across data sharing, date formats, contact number formats, mapping logic, etc. There should be standard ways to for identifier sanity checks, data quality checks, model construct development, scheduling and operationalization.


&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Output Consumption&lt;/li&gt;&lt;/b&gt;
Big data analytics motivates near real-time consumption. The traditional file sharing of scores is no longer preferred. Applications require access to analytical outputs as they happen and also on a request basis. Based on the needs, new consumption patterns are defining different mechanisms - FTP, RDBMS store, exposed ReST services (pull) or enterprise service messaging systems (push). These consumption patterns bring along new integration challenges for Big data analytical outputs.
&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Skill sets&lt;/li&gt;&lt;/b&gt;
Big data tools are driven by open source development. This makes the tools and technologies very dynamic, with new versions and features launched frequently. Organizations need to train internal teams to adopt to newer paradigms of analytics development like distributed computing and near real time computation.

&lt;/ol&gt;
&lt;p&gt;Big data analytics is becoming an integral part of decision making across various enterprises. Despite the challenges, implementation of Big data analytics is believed to be the key success criteria for organizations. It has always  been about the most important V - ‘V’ALUE.&lt;/p&gt;


    </content>
  </entry>
  
  
</feed>