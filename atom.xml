<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>Vishal Rajpal</title>
  <generator uri="https://github.com/jekyll/jekyll">Jekyll v3.1.1</generator>
    <icon>http://vishalrajpal25.github.io/apple-touch-icon-precomposed.png</icon>
  <subtitle>Big data, Hadoop, Spark, moving to Chicago, fitness</subtitle>
  <link href="http://vishalrajpal25.github.io/atom.xml" rel="self"/>
  <link href="http://vishalrajpal25.github.io/" rel="alternate" type="text/html"/>
  <updated>2016-02-25T22:51:47-06:00</updated>
  <id>http://vishalrajpal25.github.io/</id>
  <author>
    <name>Vishal Rajpal</name>
    <uri>http://vishalrajpal25.github.io/</uri>
    <email>vishalrajpal25@gmail.com</email>
  </author>

  
  <entry>
    <title>Spark on YARN - Considerations and Configurations</title>
    <link href="http://vishalrajpal25.github.io/bigdata/spark-on-yarn-considerations-and-configurations/"/>
    <updated>2016-02-25T15:29:05-06:00</updated>
    <id>http://vishalrajpal25.github.io/bigdata/spark-on-yarn-considerations-and-configurations</id>
    <author>
      <name>Vishal Rajpal</name>
      <uri>http://vishalrajpal25.github.io/</uri>
      <email>vishalrajpal25@gmail.com</email>
    </author>
    <content type="html">
      
      &lt;h5&gt;About Spark&lt;/h5&gt;
&lt;p&gt;Apache Spark™ is a DAG based execution engine for large-scale data processing. It has in-memory processing capabilities making it faster as compared to other execution engines. Spark boasts ease of use and generality through native language support (R, Scala, Python, Java ) and SQL &amp;amp; streaming analytics respectively.&lt;/p&gt;

&lt;h5&gt;Spark on Yarn&lt;/h5&gt;
&lt;p&gt;Spark runs on Hadoop (YARN), Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.
Running Spark on YARN cluster processng of data available in HDFS as well as leveraging the hardware resources available in the already commissioned Hadoop cluster.&lt;/p&gt;

&lt;p&gt;Spark documentation has detailed information about running Spark engine on YARN &lt;a href=&quot;http://spark.apache.org/docs/latest/running-on-yarn.html&quot; about=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As part of the article, I intend to expand on few of the important considerations and configurations to be aware about during practical usage of Spark for Machine learning or general purpose processing. 
I had the opportunity to play around these configurations and understand the considerations while I was involved in developing a machine learning model on text data. The versions used were: Spark 1.2, Python 2.6 and YARN.&lt;/p&gt;

&lt;ul&gt;
&lt;b&gt;&lt;li&gt;Hadoop environment variables&lt;/li&gt;&lt;/b&gt;
For Spark to run on YARN and also access the HDFS directories, it needs to know the Hadoop and Yarn client properties - basically, the resource manager (RM) address and also the HDfS port.
Below are the environment variables that need to be set to the directory containing the necessary configurations:
&lt;br /&gt;
export HADOOP_CONF_DIR=HADOOP_CONF_DIR
&lt;br /&gt;
export YARN_CONF_DIR=YARN_CONF_DIR
&lt;br /&gt;
&lt;br /&gt;

&lt;b&gt;&lt;li&gt;Initializing Spark on YARN&lt;/li&gt;&lt;/b&gt;
For initializing Spark on YARN, one needs to set the parameter --master to value as yarn. Within YARN, there are two modes - client and cluster. The modes have one significant difference pertaining to the control and execution of the driver program for the Spark application - whether within the client program OR AM (application master) in YARN.
&lt;code&gt;$ $SPARK_BIN/spark-submit --master yarn .... &lt;/code&gt;

&lt;br /&gt;
&lt;br /&gt;

&lt;b&gt;&lt;li&gt;Imports and parameters&lt;/li&gt;&lt;/b&gt;
Spark is a distributed computing engine, where multiple machines are used to run executors working on different tasks. These tasks are part of a single application controlled by the driver. As the executors run tasks in parallel, it is imperative that all executors running on machines across the cluster have access to the configurations, base code, libraries and user defined modules used in the application code. Spark provides multiple parameters to share Java maven, python modules and libraries to all executors part of the launching application. These files can be passed from the client script launching the spark application. The parameters are listed below: &lt;br /&gt;
  &lt;ol&gt;
  &lt;li&gt;--packages              Comma-separated list of maven coordinates of jars &lt;/li&gt;
  &lt;li&gt;--py-files PY_FILES     Comma-separated list of .zip, .egg, or .py files, one might always have some proprietary functions defined and used in the application&lt;/li&gt;
  &lt;li&gt;--files FILES           Comma-separated list of files, general purpose files, dictionaries, etc.&lt;/li&gt;
  &lt;li&gt;--conf PROP=VALUE       Arbitrary Spark configuration property, VM settings, etc.&lt;/li&gt;
  &lt;li&gt;&lt;i&gt;--properties-file**&lt;/i&gt;  Override spark specific properties, like memory limits&lt;/li&gt;
  &lt;/ol&gt;                         
&lt;br /&gt;

&lt;b&gt;&lt;li&gt;Memory related properties&lt;/li&gt;&lt;/b&gt;
Spark provides a number of properties for fine-tuning or changing default configurations. The primary ones that we need for large scale data processing are related to the memory settings of executor and drivers (listed below).

&lt;ol&gt;

&lt;li&gt;driver-memory ::- Total memory for driver (e.g. 1000M, 2G), container level for YARN&lt;/li&gt;
&lt;li&gt;executor-memory  ::- Total nemory per executor (e.g. 1000M, 2G), container level for YARN&lt;/li&gt;
&lt;li&gt;spark.yarn.executor.memoryOverhead ::- Typically 6-10% of executorMemory, with minimum of 384 MB. This is the amount of off-heap memory (in megabytes) to be allocated per executor. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the executor size (typically 6-10%). &lt;/li&gt;
&lt;li&gt;spark.yarn.driver.memoryOverhead ::- Typically 6-10% of driverMemory, with minimum of 384 MB. This is the amount of off-heap memory (in megabytes) to be allocated per driver in cluster mode. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the container size. &lt;/li&gt;
&lt;li&gt; spark.yarn.am.extraJavaOptions ::- A string of extra JVM options to pass to the YARN Application Master in client mode. One can use it to pass gC level setting or debugging parameters.&lt;/li&gt;


&lt;h5&gt;Closing&lt;/h5&gt;
Spark is a powerful in-memory data processing engine with a rich API and advanced data processing capabilities like graph and machine learning. One does need to understand the right combination of Spark configurations and their tuning criteria to be able to pre-process, process and train a model on large data sets.
A typical pyspark of scala spark launcher command might look like (based on the content of the article)::
&lt;br /&gt;
&lt;code&gt;pyspark --master yarn --py-files pre_processing.py --conf spark.driver.maxResultSize=1g --num-executors 10 --driver-memory 1g --executor-memory 768m &lt;/code&gt;


&lt;/ol&gt;&lt;/ul&gt;

    </content>
  </entry>
  
  <entry>
    <title>2016 - Practical Challenges in Big Data Implementations</title>
    <link href="http://vishalrajpal25.github.io/bigdata/2016-real-challenges-in-big-data-implementations/"/>
    <updated>2016-02-23T11:10:06-06:00</updated>
    <id>http://vishalrajpal25.github.io/bigdata/2016-real-challenges-in-big-data-implementations</id>
    <author>
      <name>Vishal Rajpal</name>
      <uri>http://vishalrajpal25.github.io/</uri>
      <email>vishalrajpal25@gmail.com</email>
    </author>
    <content type="html">
      
      &lt;p&gt;We are all aware of the journey of “Big data analytics” from inception until 2015. 
As we start a new year 2016, there is a different set of challenges to overcome. These challenges are primarily related to productionizing Big data projects, a.k.a. leveraging the Big data infrastructure for value creation.&lt;/p&gt;

&lt;h5&gt;Until 2015&lt;/h5&gt;
&lt;p&gt;By the end of 2015, most organizations would have already invested in some sort of (cloud or on-premises) Big data infrastructure.  They would have also initiated or completed a few proof-of-concept projects.
The primary question that needs answer for the C-suite of these organizations (IMHO) is and would always be - “How do we scale the value beyond the 3 Vs?”.&lt;/p&gt;

&lt;h5&gt;2016 &amp;amp; beyond&lt;/h5&gt;
&lt;p&gt;2016 brings the new phase of advanced implementations to productionize analytical solutions in the Big data infrastructure.
Below are few of the challenges that I think would be part of (atleast) the 2016 journey.&lt;/p&gt;

&lt;ul style=&quot;list-style-type:disc&quot;&gt;
  &lt;h6&gt;&lt;li&gt;Strategy&lt;/li&gt;&lt;/h6&gt;
  Big data space is known to be very dynamic, with many moving parts. New tools / stores are launched frequently. Hadoop ecosystem itself is evolving everyday and there is a huge open source development community releasing new tools / features. 
  In such an environment, organizations need to be very dynamic and need to re-visit their strategy more often. Also, ensuring internal teams build new skill-sets as they scale should also be part of the strategy in the near term.
  &lt;h6&gt;&lt;li&gt;Infrastructure&lt;/li&gt;&lt;/h6&gt;
  The concept of different environments is yet to mature for Big data ecosystem. There is definitely a need for separate environments for development, staging / testing and production, but the challenge would be to acquire, plug and commission the right infrastructure to roll out within the organization. There is a fair amount of estimation required to determine the workload  that the environment should support in a longer term, selecting versions of VMs&#39; &amp;amp; tools and also network layouts of these environments since they need to be integrated within the enterprise data centers and systems.
  &lt;h6&gt;&lt;li&gt;Security &amp;amp; Governance&lt;/li&gt;&lt;/h6&gt;
  There are a few mechanisms available for securing the Big data environment - network, storage encryption and access related. Integrating the right tools with the enterprise directory along with the different authentication protocols needs investment of administrators and security team. Also, before we begin the ingestion of data in the environment, it is important to define the zones / governed areas for hosting data per the type - raw, mapped, processed, production runs and so on. The zones / folders defined need to be commissioned with appropriate policies based on access and applications.
  &lt;h6&gt;&lt;li&gt;Data availability&lt;/li&gt;&lt;/h6&gt;
  Making data available in the data lake for analysis has been one of the biggest challenges. With the fanciness of polygot persistence, this problem is not going to go away in the near future. Pulling data from system of records / RDBMS, data warehouse, raw files and MPP systems is a daunting task. Ingesting data from multiple sources / teams requires a lot of coordination and effort for project management to ensure smooth execution. The effort might require creation of external views in external systems to pull data in the cluster using ingestion tools. Also, if the analysis require maintaining a bitemporal view of the data (time slicing), it becomes much more difficult if the source systems do not have history of change or audits to maintain changes. This, in turn, might need additional efforts to create triggers and tables to maintain audit type changes.
  &lt;h6&gt;&lt;li&gt;Processes&lt;/li&gt;&lt;/h6&gt;
  There are multiple teams involved in the Big data environment. From administrators to data engineers, data analysts to data scientists and even IT developers, the need for clearly laid out processes is inevitable. Processes around new project initiation, data source pull, model development standards, data sharing and operationalization should be defined. Organization structure within an enterprise drives the processes being created and owners defined. 
  &lt;h6&gt;&lt;li&gt;Standardization&lt;/li&gt;&lt;/h6&gt;
  Data being available from disparate sources calls for standardization across data sharing, date formats, contact number formats, mapping logic, etc. Also, as data scientists develop models while IT operationalize, there should be standard ways to for identifier sanity checks, data quality checks, model construct development, scheduling and operationalization.
  &lt;h6&gt;&lt;li&gt;Consumption&lt;/li&gt;&lt;/h6&gt;
  Most analytics consumption happens within existing enterprise systems, since the model could be optimizing, predicting or enhancing some business functionality. Hence, it is imperative that the enterprise systems can easily communicate with the Big data infrastructure. This could be in a coupled way - FTP, RDBMS table or view OR could be de-coupled in the form of exposed ReST services (pull) or enterprise service messaging systems (push). 
&lt;/ul&gt;

&lt;h5&gt;Closing&lt;/h5&gt;
&lt;p&gt;This is not an exhaustive list but some challenges I could think of or have known. 
Despite the challenges, Big data is turning into a more mature and important component of enterprise systems. The adaptor of Big data analytics will definitely feel the difference versus the competition as they progress further in generating the ‘V’ALUE.&lt;/p&gt;


    </content>
  </entry>
  
  
</feed>