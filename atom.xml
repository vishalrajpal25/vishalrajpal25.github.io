<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>Vishal Rajpal</title>
  <generator uri="https://github.com/jekyll/jekyll">Jekyll v3.1.1</generator>
    <icon>http://vishalrajpal25.github.io/apple-touch-icon-precomposed.png</icon>
  <subtitle>Big data, Hadoop, Spark, moving to Chicago, fitness</subtitle>
  <link href="http://vishalrajpal25.github.io/atom.xml" rel="self"/>
  <link href="http://vishalrajpal25.github.io/" rel="alternate" type="text/html"/>
  <updated>2016-03-21T10:58:41-05:00</updated>
  <id>http://vishalrajpal25.github.io/</id>
  <author>
    <name>Vishal Rajpal</name>
    <uri>http://vishalrajpal25.github.io/</uri>
    <email>vishalrajpal25@gmail.com</email>
  </author>

  
  <entry>
    <title>Applying for a Driver&#39;s License in Chicago</title>
    <link href="http://vishalrajpal25.github.io/diy/applying-for-a-drivers-license-in-chicago/"/>
    <updated>2016-03-21T10:11:10-05:00</updated>
    <id>http://vishalrajpal25.github.io/diy/applying-for-a-drivers-license-in-chicago</id>
    <author>
      <name>Vishal Rajpal</name>
      <uri>http://vishalrajpal25.github.io/</uri>
      <email>vishalrajpal25@gmail.com</email>
    </author>
    <content type="html">
      
      &lt;p&gt;Driver’s license is one of the most important documents that one applies for in the US. Driver’s license not only is your permit to drive motor vehicles, but is a general identity document issued by your state. It contains key information like one’s full name, date of birth, social security # and address, thus can be used as a state ID as well.&lt;/p&gt;

&lt;p&gt;I recently applied for an IL state driver’s license and got it approved. I did not use any help from a learning school OR a friend (for a car), hence thought to share my “DIY” experience.&lt;/p&gt;

&lt;p&gt;Below are the steps for applying a driver’s license in DIY mode:&lt;/p&gt;
&lt;ol&gt;
&lt;b&gt;&lt;li&gt;Documents&lt;/li&gt;&lt;/b&gt;
Go to the IL DMV website and download the PDF which contains the list of documents that one needs to have for applying a permit. There are four categories and one should have 1 document each for 3 categories and 2 documents for the residential address proof. Please ensure that you have the necessary documents available, specifically I-94 in case you are an alien.

&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Written Test&lt;/li&gt;&lt;/b&gt;
Please do not take the written test lightly. Ensure to spend at least a couple of hours going through practice tests, signs and general documentation provided by the DMV. I took 4 practice tests and went through the signs in the PDF provided on the DMV website. Once you are ready, take a sample test from DMV available free with a login. On the test day, you will realize the importance of going through the material and signs. It is not that hard, but is definitely not easy as well. One needs to have some background to answer the questions correctly.

&lt;br /&gt;
&lt;br /&gt;

&lt;b&gt;&lt;li&gt;Driving Test&lt;/li&gt;&lt;/b&gt;
For the driving test, you need a vehicle of your own. The DMV does not provide a vehicle for the driving test. You may either request a friend and borrow his/her vehicle. In that case, the friend needs to accompany you to the DMV and provide his written consent to use his car for the driving test. 
For others, you may need to rent a car with the proper insurance coverage. I rented a vehicle from the ORD airport at 12 USD / day and bought a 13 USD insurance coverage on the vehicle. 
&lt;p&gt;
The driving test is again not that difficult, but do not undermine it as well. Ensure to follow all rules - wear seat belts and adjust mirrors as you sit in the car. Follow all STOP signs which means bring the car to a complete halt at the intersections having STOP signs. 
The evalutor might talk to you during the time of the drive, hence you need to be very alert while driving and communicating with them at the same time. 

&lt;br /&gt;&lt;br /&gt;
Once you successfully pass the tests, a photo is clicked and your signature is requested. That&#39;s it, you have your driver&#39;s license in your hand the same day.

&lt;/p&gt;&lt;/ol&gt;

    </content>
  </entry>
  
  <entry>
    <title>Settling in Chicago</title>
    <link href="http://vishalrajpal25.github.io/diy/settling-in-chicago/"/>
    <updated>2016-03-02T18:37:07-06:00</updated>
    <id>http://vishalrajpal25.github.io/diy/settling-in-chicago</id>
    <author>
      <name>Vishal Rajpal</name>
      <uri>http://vishalrajpal25.github.io/</uri>
      <email>vishalrajpal25@gmail.com</email>
    </author>
    <content type="html">
      
      &lt;p&gt;We (my wife and I) had a unique experience of finding an apartment, buying necessary amenities / furniture at the time we reached Chicago. I thought to document our experiences and share for people who just moved and need a guide.&lt;/p&gt;

&lt;h5&gt;Our Experiences&lt;/h5&gt;
&lt;p&gt;The first challenge began with the weather. Chicago is very cold during the winters - specifically months of January and February. I would strongly recommend to take the cold seriously and wear 4-5 layers while stepping out during cold weather.&lt;/p&gt;

&lt;p&gt;I reached Chicago in the 2nd week of January with high expectations to settle as quickly as possible. The day we reached, the weather department issued a wind chill advisory, which means temperature dropping to single digits in Fahrenheit (negative in celsius). I clearly remember temperature of -17 degrees C on the days we were visiting various apartments in Downtown Chicago.&lt;/p&gt;

&lt;h5&gt;Tips&lt;/h5&gt;
&lt;p&gt;Here are a few tips I thought to share:&lt;/p&gt;

&lt;ol&gt;
&lt;b&gt;&lt;li&gt;Pre-liminary research&lt;/li&gt;&lt;/b&gt;
Although the first step should be to identify one&#39;s budget, it does not harm in browsing around doing some research on the available apartments and their rents. One could use a number of national (apartment, rent, etc) and local (domu, craigslist) websites to check various locations and the rents in those areas,
&lt;br /&gt;&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Budget&lt;/li&gt;&lt;/b&gt;
This is very important. Vanilla case would be 35% of take home salary is the max one can afford to pay as monthly rent. I have a slightly different opinion, since you may have an option to save more and pay only 10-20% of your take home as rent. Also, none of the websites I browsed mentioned about the added costs related to renting an apartment. These are utility bills, renters insurance, one time internet cable connection charges to name a few. Ensure that your budget is inclusive of all the monthly pay-outs apart from the rent to be able to completely visualize your cost of renting the apartment
&lt;br /&gt;&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Location&lt;/li&gt;&lt;/b&gt;
One of the most important criteria is whether to rent in Chicago downtown or suburbs. There are limitless options for one to rent an apartment. Some folks whom I spoke to try to find locations closer to their friends, relatives, god-fathers or god-brothers. Post that, they looked at their location of work. I would recommend both - try to find an apartment closest to work and also reachable to acquaintances. Both are equally important. You may find spending 200 USD more on an apartment closer to work and getting more time with family worthwhile as opposed to saving the 200 USD (which eventually you may spend in buying Metra tickets). Nevertheless, the right location is crucial, save money as well time.
&lt;br /&gt;
Downtown - high rents (-), no need to maintain a car (-), very costly parking (-), easily approachable (+), events and city life (+)
&lt;br /&gt;
Suburbs - low cost of living including rents and grocery (+), free / cheap parking (+), travel time to Chicago city (-)
&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Agents&lt;/li&gt;&lt;/b&gt;
There are some very knowledgable agents who can help you find you the right apartment. What you need to do is call them, provide your budget, preferred location, office location, parking needs, schooling needs and the like. Just share whatever is your criteria and they will take care of the rest. They will necessarily short list 3-4 options, take appointments, drive you around the 4 places and help you select the one.
P.S. Their service is free of cost, they charge the building management for their services on every apartment they help lease out. 
I find it very cool!

&lt;/ol&gt;

&lt;p&gt;Once you finalize the apartment, below are some tips related to setting up your place:&lt;/p&gt;
&lt;ol&gt;
&lt;b&gt;&lt;li&gt;Renter&#39;s agreement&lt;/li&gt;&lt;/b&gt;
Provide necessary documents - passport, visa and family details and the building authorities take care of creating the agreement. It is an electronic agreement, so you need to just go through it and sign electronically.
&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Renter&#39;s insurance&lt;/li&gt;&lt;/b&gt;
It is a pre-requisite to take renters insurance before the possession is handed over to you. It costs around 25-30 USD and is straightforward. You can pick one of the insurance amount specified in your agreement.
&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Advance rent&lt;/li&gt;&lt;/b&gt;
Prepare money order for the advance rent for the first month. I used US Postal service given that it was cheaper as compared to banks.
&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;&lt;li&gt;Necessities&lt;/li&gt;&lt;/b&gt;
Buy a NEW Mattress and bed, refrain from buying a used one. Other furniture can be bought used through craigslist OR any local furniture store or website. Electronics like Smart TV as well can be bought used but I bought a new one (that&#39;s my preference).
&lt;/ol&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
Hope the above is helpful. If you have any other specific questions around settling in Chicago, drop me a note and I would be glad to help.&lt;/p&gt;


    </content>
  </entry>
  
  <entry>
    <title>Spark on YARN - Considerations and Configurations</title>
    <link href="http://vishalrajpal25.github.io/bigdata/spark-on-yarn-considerations-and-configurations/"/>
    <updated>2016-02-25T15:29:05-06:00</updated>
    <id>http://vishalrajpal25.github.io/bigdata/spark-on-yarn-considerations-and-configurations</id>
    <author>
      <name>Vishal Rajpal</name>
      <uri>http://vishalrajpal25.github.io/</uri>
      <email>vishalrajpal25@gmail.com</email>
    </author>
    <content type="html">
      
      &lt;h5&gt;About Spark&lt;/h5&gt;
&lt;p&gt;Apache Spark™ is a DAG based execution engine for large-scale data processing. It has in-memory processing capabilities making it faster as compared to other execution engines. Spark boasts ease of use and generality through native language support (R, Scala, Python, Java ) and SQL &amp;amp; streaming analytics respectively.&lt;/p&gt;

&lt;h5&gt;Spark on Yarn&lt;/h5&gt;
&lt;p&gt;Spark runs on Hadoop (YARN), Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.
Running Spark on YARN cluster processng of data available in HDFS as well as leveraging the hardware resources available in the already commissioned Hadoop cluster.&lt;/p&gt;

&lt;p&gt;Spark documentation has detailed information about running Spark engine on YARN &lt;a href=&quot;http://spark.apache.org/docs/latest/running-on-yarn.html&quot; about=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As part of the article, I intend to expand on few of the important considerations and configurations to be aware about during practical usage of Spark for Machine learning or general purpose processing. 
I had the opportunity to play around these configurations and understand the considerations while I was involved in developing a machine learning model on text data. The versions used were: Spark 1.2, Python 2.6 and YARN.&lt;/p&gt;

&lt;ul&gt;
&lt;b&gt;&lt;li&gt;Hadoop environment variables&lt;/li&gt;&lt;/b&gt;
For Spark to run on YARN and also access the HDFS directories, it needs to know the Hadoop and Yarn client properties - basically, the resource manager (RM) address and also the HDfS port.
Below are the environment variables that need to be set to the directory containing the necessary configurations:
&lt;br /&gt;
export HADOOP_CONF_DIR=HADOOP_CONF_DIR
&lt;br /&gt;
export YARN_CONF_DIR=YARN_CONF_DIR
&lt;br /&gt;
&lt;br /&gt;

&lt;b&gt;&lt;li&gt;Initializing Spark on YARN&lt;/li&gt;&lt;/b&gt;
For initializing Spark on YARN, one needs to set the parameter --master to value as yarn. Within YARN, there are two modes - client and cluster. The modes have one significant difference pertaining to the control and execution of the driver program for the Spark application - whether within the client program OR AM (application master) in YARN.
&lt;code&gt;$ $SPARK_BIN/spark-submit --master yarn .... &lt;/code&gt;

&lt;br /&gt;
&lt;br /&gt;

&lt;b&gt;&lt;li&gt;Imports and parameters&lt;/li&gt;&lt;/b&gt;
Spark is a distributed computing engine, where multiple machines are used to run executors working on different tasks. These tasks are part of a single application controlled by the driver. As the executors run tasks in parallel, it is imperative that all executors running on machines across the cluster have access to the configurations, base code, libraries and user defined modules used in the application code. Spark provides multiple parameters to share Java maven, python modules and libraries to all executors part of the launching application. These files can be passed from the client script launching the spark application. The parameters are listed below: &lt;br /&gt;
  &lt;ol&gt;
  &lt;li&gt;--packages              Comma-separated list of maven coordinates of jars &lt;/li&gt;
  &lt;li&gt;--py-files PY_FILES     Comma-separated list of .zip, .egg, or .py files, one might always have some proprietary functions defined and used in the application&lt;/li&gt;
  &lt;li&gt;--files FILES           Comma-separated list of files, general purpose files, dictionaries, etc.&lt;/li&gt;
  &lt;li&gt;--conf PROP=VALUE       Arbitrary Spark configuration property, VM settings, etc.&lt;/li&gt;
  &lt;li&gt;&lt;i&gt;--properties-file**&lt;/i&gt;  Override spark specific properties, like memory limits&lt;/li&gt;
  &lt;/ol&gt;                         
&lt;br /&gt;

&lt;b&gt;&lt;li&gt;Memory related properties&lt;/li&gt;&lt;/b&gt;
Spark provides a number of properties for fine-tuning or changing default configurations. The primary ones that we need for large scale data processing are related to the memory settings of executor and drivers (listed below).

&lt;ol&gt;

&lt;li&gt;driver-memory ::- Total memory for driver (e.g. 1000M, 2G), container level for YARN&lt;/li&gt;
&lt;li&gt;executor-memory  ::- Total nemory per executor (e.g. 1000M, 2G), container level for YARN&lt;/li&gt;
&lt;li&gt;spark.yarn.executor.memoryOverhead ::- Typically 6-10% of executorMemory, with minimum of 384 MB. This is the amount of off-heap memory (in megabytes) to be allocated per executor. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the executor size (typically 6-10%). &lt;/li&gt;
&lt;li&gt;spark.yarn.driver.memoryOverhead ::- Typically 6-10% of driverMemory, with minimum of 384 MB. This is the amount of off-heap memory (in megabytes) to be allocated per driver in cluster mode. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the container size. &lt;/li&gt;
&lt;li&gt; spark.yarn.am.extraJavaOptions ::- A string of extra JVM options to pass to the YARN Application Master in client mode. One can use it to pass gC level setting or debugging parameters.&lt;/li&gt;


&lt;h5&gt;Closing&lt;/h5&gt;
Spark is a powerful in-memory data processing engine with a rich API and advanced data processing capabilities like graph and machine learning. One does need to understand the right combination of Spark configurations and their tuning criteria to be able to pre-process, process and train a model on large data sets.
A typical pyspark of scala spark launcher command might look like (based on the content of the article)::
&lt;br /&gt;
&lt;code&gt;pyspark --master yarn --py-files pre_processing.py --conf spark.driver.maxResultSize=1g --num-executors 10 --driver-memory 1g --executor-memory 768m &lt;/code&gt;


&lt;/ol&gt;&lt;/ul&gt;

    </content>
  </entry>
  
  <entry>
    <title>2016 - Practical Challenges in Big Data Implementations</title>
    <link href="http://vishalrajpal25.github.io/bigdata/2016-real-challenges-in-big-data-implementations/"/>
    <updated>2016-02-23T11:10:06-06:00</updated>
    <id>http://vishalrajpal25.github.io/bigdata/2016-real-challenges-in-big-data-implementations</id>
    <author>
      <name>Vishal Rajpal</name>
      <uri>http://vishalrajpal25.github.io/</uri>
      <email>vishalrajpal25@gmail.com</email>
    </author>
    <content type="html">
      
      &lt;p&gt;We are all aware of the journey of “Big data analytics” from inception until 2015. 
As we start a new year 2016, there is a different set of challenges to overcome. These challenges are primarily related to productionizing Big data projects, a.k.a. leveraging the Big data infrastructure for value creation.&lt;/p&gt;

&lt;h5&gt;Until 2015&lt;/h5&gt;
&lt;p&gt;By the end of 2015, most organizations would have already invested in some sort of (cloud or on-premises) Big data infrastructure.  They would have also initiated or completed a few proof-of-concept projects.
The primary question that needs answer for the C-suite of these organizations (IMHO) is and would always be - “How do we scale the value beyond the 3 Vs?”.&lt;/p&gt;

&lt;h5&gt;2016 &amp;amp; beyond&lt;/h5&gt;
&lt;p&gt;2016 brings the new phase of advanced implementations to productionize analytical solutions in the Big data infrastructure.
Below are few of the challenges that I think would be part of (atleast) the 2016 journey.&lt;/p&gt;

&lt;ul style=&quot;list-style-type:disc&quot;&gt;
  &lt;h6&gt;&lt;li&gt;Strategy&lt;/li&gt;&lt;/h6&gt;
  Big data space is known to be very dynamic, with many moving parts. New tools / stores are launched frequently. Hadoop ecosystem itself is evolving everyday and there is a huge open source development community releasing new tools / features. 
  In such an environment, organizations need to be very dynamic and need to re-visit their strategy more often. Also, ensuring internal teams build new skill-sets as they scale should also be part of the strategy in the near term.
  &lt;h6&gt;&lt;li&gt;Infrastructure&lt;/li&gt;&lt;/h6&gt;
  The concept of different environments is yet to mature for Big data ecosystem. There is definitely a need for separate environments for development, staging / testing and production, but the challenge would be to acquire, plug and commission the right infrastructure to roll out within the organization. There is a fair amount of estimation required to determine the workload  that the environment should support in a longer term, selecting versions of VMs&#39; &amp;amp; tools and also network layouts of these environments since they need to be integrated within the enterprise data centers and systems.
  &lt;h6&gt;&lt;li&gt;Security &amp;amp; Governance&lt;/li&gt;&lt;/h6&gt;
  There are a few mechanisms available for securing the Big data environment - network, storage encryption and access related. Integrating the right tools with the enterprise directory along with the different authentication protocols needs investment of administrators and security team. Also, before we begin the ingestion of data in the environment, it is important to define the zones / governed areas for hosting data per the type - raw, mapped, processed, production runs and so on. The zones / folders defined need to be commissioned with appropriate policies based on access and applications.
  &lt;h6&gt;&lt;li&gt;Data availability&lt;/li&gt;&lt;/h6&gt;
  Making data available in the data lake for analysis has been one of the biggest challenges. With the fanciness of polygot persistence, this problem is not going to go away in the near future. Pulling data from system of records / RDBMS, data warehouse, raw files and MPP systems is a daunting task. Ingesting data from multiple sources / teams requires a lot of coordination and effort for project management to ensure smooth execution. The effort might require creation of external views in external systems to pull data in the cluster using ingestion tools. Also, if the analysis require maintaining a bitemporal view of the data (time slicing), it becomes much more difficult if the source systems do not have history of change or audits to maintain changes. This, in turn, might need additional efforts to create triggers and tables to maintain audit type changes.
  &lt;h6&gt;&lt;li&gt;Processes&lt;/li&gt;&lt;/h6&gt;
  There are multiple teams involved in the Big data environment. From administrators to data engineers, data analysts to data scientists and even IT developers, the need for clearly laid out processes is inevitable. Processes around new project initiation, data source pull, model development standards, data sharing and operationalization should be defined. Organization structure within an enterprise drives the processes being created and owners defined. 
  &lt;h6&gt;&lt;li&gt;Standardization&lt;/li&gt;&lt;/h6&gt;
  Data being available from disparate sources calls for standardization across data sharing, date formats, contact number formats, mapping logic, etc. Also, as data scientists develop models while IT operationalize, there should be standard ways to for identifier sanity checks, data quality checks, model construct development, scheduling and operationalization.
  &lt;h6&gt;&lt;li&gt;Consumption&lt;/li&gt;&lt;/h6&gt;
  Most analytics consumption happens within existing enterprise systems, since the model could be optimizing, predicting or enhancing some business functionality. Hence, it is imperative that the enterprise systems can easily communicate with the Big data infrastructure. This could be in a coupled way - FTP, RDBMS table or view OR could be de-coupled in the form of exposed ReST services (pull) or enterprise service messaging systems (push). 
&lt;/ul&gt;

&lt;h5&gt;Closing&lt;/h5&gt;
&lt;p&gt;This is not an exhaustive list but some challenges I could think of or have known. 
Despite the challenges, Big data is turning into a more mature and important component of enterprise systems. The adaptor of Big data analytics will definitely feel the difference versus the competition as they progress further in generating the ‘V’ALUE.&lt;/p&gt;


    </content>
  </entry>
  
  
</feed>